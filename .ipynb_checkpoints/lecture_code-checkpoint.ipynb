{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22bab757-3d99-4a6e-a4e6-9211d2b0d3e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check\n"
     ]
    }
   ],
   "source": [
    "import torch  # Core PyTorch library for tensor operations and neural networks\n",
    "import torch.nn as nn  # Neural network module from PyTorch\n",
    "import torch.optim as optim  # Optimization algorithms (like Adam, SGD)\n",
    "from torch.utils.data import Dataset, DataLoader, random_split  # Data utilities\n",
    "from torchvision import transforms  # Image transformations\n",
    "from PIL import Image  # Python Imaging Library for image handling\n",
    "import os  # Operating system interface for file operations\n",
    "import matplotlib.pyplot as plt  # Plotting library for visualizing results\n",
    "import numpy as np\n",
    "\n",
    "# =============================================================================\n",
    "# Custom Dataset Class for Image Super-Resolution\n",
    "# =============================================================================\n",
    "\n",
    "class ImageSuperResDataset(Dataset):\n",
    "    def __init__(self, input_dir, output_dir):\n",
    "        \"\"\"\n",
    "        Initialize the dataset with directories containing low-res and high-res images.\n",
    "        \n",
    "        Args:\n",
    "            input_dir: Directory containing low-resolution input images (225x300)\n",
    "            output_dir: Directory corresponding high-resolution target images (450x600)\n",
    "        \"\"\"\n",
    "        self.input_dir = input_dir\n",
    "        self.output_dir = output_dir\n",
    "        # Get sorted list of filenames to ensure consistent ordering\n",
    "        self.filenames = sorted(os.listdir(input_dir))\n",
    "        \n",
    "        # Define transformations for input and output images\n",
    "        self.input_transform = transforms.Compose([\n",
    "            transforms.Resize((225, 300)),  # Resize all inputs to standard size (225x300)\n",
    "            transforms.ToTensor()  # Convert PIL image to PyTorch tensor\n",
    "        ])\n",
    "        self.output_transform = transforms.Compose([\n",
    "            transforms.Resize((450, 600)),  # Resize all outputs to double the input dimensions (450x600)\n",
    "            transforms.ToTensor()  # Convert PIL image to PyTorch tensor\n",
    "        ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the total number of images in the dataset\"\"\"\n",
    "        return len(self.filenames)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieve a pair of low-res and high-res images by index.\n",
    "        \n",
    "        Args:\n",
    "            idx: Index of the image to retrieve\n",
    "            \n",
    "        Returns:\n",
    "            A tuple containing (input_tensor, output_tensor) where input is 3x225x300\n",
    "            and output is 3x450x600 (channels x height x width)\n",
    "        \"\"\"\n",
    "        img_name = self.filenames[idx]\n",
    "        \n",
    "        # Load low-resolution input image\n",
    "        input_path = os.path.join(self.input_dir, img_name)\n",
    "        input_img = Image.open(input_path).convert('RGB')  # Ensure RGB format\n",
    "        input_tensor = self.input_transform(input_img)\n",
    "        \n",
    "        # Load high-resolution target image\n",
    "        output_path = os.path.join(self.output_dir, img_name)\n",
    "        output_img = Image.open(output_path).convert('RGB')  # Ensure RGB format\n",
    "        output_tensor = self.output_transform(output_img)\n",
    "        \n",
    "        # Return images as 2D tensors (C x H x W) without flattening\n",
    "        return input_tensor, output_tensor\n",
    "    \n",
    "\n",
    "# =============================================================================\n",
    "# VISUALIZATION FUNCTION\n",
    "# =============================================================================\n",
    "def visualize_super_resolution(model, test_loader, device, num_examples=5):\n",
    "    \"\"\"\n",
    "    Visualize super-resolution results with side-by-side comparisons.\n",
    "    \n",
    "    For each example, we show:\n",
    "    1. Input: Low-resolution image (225x300)\n",
    "    2. Model Output: Super-resolved image (450x600)\n",
    "    3. Ground Truth: Actual high-resolution image (450x600)\n",
    "    \n",
    "    Args:\n",
    "        model: The trained super-resolution model\n",
    "        test_loader: DataLoader containing test images\n",
    "        num_examples: Number of examples to visualize (default: 3)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    data_iter = iter(test_loader)\n",
    "    inputs, targets = next(data_iter)\n",
    "    \n",
    "    # Limit to requested number of examples and move to device\n",
    "    inputs = inputs[:num_examples].to(device)\n",
    "    targets = targets[:num_examples].to(device)\n",
    "    \n",
    "    print(f\"\\nGenerating super-resolution for {num_examples} test images...\")\n",
    "    \n",
    "    # Generate predictions (no gradient computation needed for inference)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(inputs)\n",
    "    \n",
    "    # Move tensors to CPU and convert to numpy arrays for plotting\n",
    "    inputs_np = inputs.cpu().numpy()\n",
    "    outputs_np = outputs.cpu().numpy()\n",
    "    targets_np = targets.cpu().numpy()\n",
    "    \n",
    "    # Create a grid of subplots: one row per example, three columns per row\n",
    "    fig, axes = plt.subplots(num_examples, 3, figsize=(15, 5 * num_examples))\n",
    "    \n",
    "    # Handle case of single example (axes would be 1D)\n",
    "    if num_examples == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    # Process each example\n",
    "    for i in range(num_examples):\n",
    "        # Convert from PyTorch format (C, H, W) to matplotlib format (H, W, C)\n",
    "        input_img = np.transpose(inputs_np[i], (1, 2, 0))\n",
    "        output_img = np.transpose(outputs_np[i], (1, 2, 0))\n",
    "        target_img = np.transpose(targets_np[i], (1, 2, 0))\n",
    "        \n",
    "        # Ensure pixel values are in valid range [0, 1]\n",
    "        input_img = np.clip(input_img, 0, 1)\n",
    "        output_img = np.clip(output_img, 0, 1)\n",
    "        target_img = np.clip(target_img, 0, 1)\n",
    "        \n",
    "        # Display the three images side by side\n",
    "        axes[i, 0].imshow(input_img)\n",
    "        axes[i, 0].set_title(f'Input (Low-Res)\\n{inputs_np[i].shape[1]}x{inputs_np[i].shape[2]}')\n",
    "        axes[i, 0].axis('off')\n",
    "        \n",
    "        axes[i, 1].imshow(output_img)\n",
    "        axes[i, 1].set_title(f'Model Output\\n{outputs_np[i].shape[1]}x{outputs_np[i].shape[2]}')\n",
    "        axes[i, 1].axis('off')\n",
    "        \n",
    "        axes[i, 2].imshow(target_img)\n",
    "        axes[i, 2].set_title(f'Ground Truth\\n{targets_np[i].shape[1]}x{targets_np[i].shape[2]}')\n",
    "        axes[i, 2].axis('off')\n",
    "        \n",
    "        # Calculate quality metrics\n",
    "        mse = np.mean((output_img - target_img) ** 2)\n",
    "        psnr = 10 * np.log10(1.0 / (mse + 1e-10))  # Add small epsilon to avoid division by zero\n",
    "        \n",
    "        print(f\"  Example {i+1} - MSE: {mse:.6f}, PSNR: {psnr:.2f} dB\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    #plt.savefig('superres_results.png', dpi=150, bbox_inches='tight')\n",
    "    #print(f\"\\nVisualization saved as 'superres_results.png'\")\n",
    "    plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# QUANTITATIVE EVALUATION FUNCTION\n",
    "# =============================================================================\n",
    "def evaluate_metrics(model, test_loader, device):\n",
    "    \"\"\"\n",
    "    Calculate average quality metrics across the entire test set.\n",
    "    \n",
    "    Metrics:\n",
    "    - MSE (Mean Squared Error): Lower is better, measures pixel-wise difference\n",
    "    - PSNR (Peak Signal-to-Noise Ratio): Higher is better, logarithmic quality measure\n",
    "    \n",
    "    Args:\n",
    "        model: The trained super-resolution model\n",
    "        test_loader: DataLoader containing all test images\n",
    "        \n",
    "    Returns:\n",
    "        avg_mse: Average mean squared error\n",
    "        avg_psnr: Average PSNR in decibels\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Accumulate metrics across all batches\n",
    "    total_mse = 0\n",
    "    total_psnr = 0\n",
    "    count = 0\n",
    "    \n",
    "    print(\"\\nEvaluating model on test set...\")\n",
    "    \n",
    "    # Process all test images\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            # Generate predictions\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Calculate MSE and PSNR for each image in the batch\n",
    "            mse = torch.mean((outputs - targets) ** 2, dim=[1, 2, 3])\n",
    "            psnr = 10 * torch.log10(1.0 / (mse + 1e-10))\n",
    "            \n",
    "            # Accumulate statistics\n",
    "            total_mse += mse.sum().item()\n",
    "            total_psnr += psnr.sum().item()\n",
    "            count += inputs.size(0)\n",
    "            \n",
    "            # Print progress every 10 batches\n",
    "            if (batch_idx + 1) % 10 == 0:\n",
    "                print(f\"  Processed {count} images...\")\n",
    "    \n",
    "    # Calculate averages\n",
    "    avg_mse = total_mse / count\n",
    "    avg_psnr = total_psnr / count\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"TEST SET EVALUATION RESULTS\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Total images evaluated: {count}\")\n",
    "    print(f\"Average MSE:  {avg_mse:.6f}\")\n",
    "    print(f\"Average PSNR: {avg_psnr:.2f} dB\")\n",
    "    \n",
    "    # Simple quality assessment\n",
    "    if avg_psnr >= 30:\n",
    "        quality = \"Good\"\n",
    "    else:\n",
    "        quality = \"Better hyperparameter search needed\"\n",
    "        \n",
    "    print(f\"Quality Assessment: {quality}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    return avg_mse, avg_psnr\n",
    "print(\"check\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b3dbfe0-612b-4541-b436-45ce7005a592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check\n",
      "Using device: cpu\n",
      "BF16 enabled: False\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "=============================================================================\n",
    "SUPER-RESOLUTION MODEL EVALUATION\n",
    "=============================================================================\n",
    "This notebook demonstrates how to:\n",
    "1. Load a trained super-resolution neural network\n",
    "2. Test it on unseen data\n",
    "3. Visualize the results with before/after comparisons\n",
    "4. Calculate quantitative metrics (MSE and PSNR)\n",
    "\n",
    "Super-resolution is the task of taking a low-resolution image and generating\n",
    "a high-resolution version. Our model takes 225x300 images and outputs 450x600\n",
    "images (2x upscaling).\n",
    "\n",
    "A 2GB VRAM GPU is enough for the neural network\n",
    "=============================================================================\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Import necessary libraries for deep learning, image processing, and visualization\n",
    "import torch  # Core PyTorch library for tensor operations and neural networks\n",
    "import torch.nn as nn  # Neural network module from PyTorch\n",
    "import torch.optim as optim  # Optimization algorithms (like Adam, SGD)\n",
    "from torch.utils.data import Dataset, DataLoader, random_split  # Data utilities\n",
    "from torchvision import transforms  # Image transformations\n",
    "from PIL import Image  # Python Imaging Library for image handling\n",
    "import os  # Operating system interface for file operations\n",
    "import myutilities\n",
    "\n",
    "\n",
    "# Set device and enable BF16 if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # Use GPU if available, otherwise CPU\n",
    "use_bf16 = torch.cuda.is_available() and torch.cuda.is_bf16_supported()  # Check if BF16 (bfloat16) is supported on the GPU\n",
    "print(f\"Using device: {device}\")  # Print which device is being used\n",
    "print(f\"BF16 enabled: {use_bf16}\")  # Print whether BF16 is enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a0e7378c-f55a-43ae-92b9-7bfdb8a2a18c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Test samples: 383\n"
     ]
    }
   ],
   "source": [
    "# Load and prepare the dataset\n",
    "print(\"Loading dataset...\")\n",
    "dataset = myutilities.ImageSuperResDataset('kaggle/train_x', 'kaggle/train_y')\n",
    "\n",
    "# Split dataset into training, validation, and test sets (70%/15%/15%)\n",
    "total_size = len(dataset)\n",
    "train_size = int(0.70 * total_size)  # 70% for training\n",
    "val_size = int(0.15 * total_size)    # 15% for validation\n",
    "test_size = total_size - train_size - val_size  # Remaining 15% for testing\n",
    "\n",
    "# Use random_split to create subsets and set a seed for reproducibility\n",
    "# We do not need the train and validation data now\n",
    "_, _, test_dataset = random_split(\n",
    "    dataset, [train_size, val_size, test_size],\n",
    "    generator=torch.Generator().manual_seed(42)  # Fixed seed for consistent splits\n",
    ")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "\n",
    "# Create data loaders for efficient batch processing\n",
    "batch_size = 10  # Number of images to process in each batch (adjust based on GPU memory)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "66535a5b-fc9d-4326-a21d-4470bd7720b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 225, 300])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4edbf3a3-2fac-43ab-972c-c7f4d931cc45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SETUP: Device Configuration\n",
    "# =============================================================================\n",
    "# Check if GPU is available - neural networks run much faster on GPUs\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# =============================================================================\n",
    "# MODEL ARCHITECTURE DEFINITIONS\n",
    "# =============================================================================\n",
    "\n",
    "# =============================================================================\n",
    "# CUSTOM PIXEL SHUFFLE IMPLEMENTATION\n",
    "# =============================================================================\n",
    "class CustomPixelShuffle(nn.Module):\n",
    "    \"\"\"\n",
    "    Hand-made implementation of PixelShuffle (also known as sub-pixel convolution).\n",
    "    PyTorch has an implementation of it called nn.PixelShuffle\n",
    "    \n",
    "    This operation rearranges elements in a tensor from depth to spatial dimensions.\n",
    "    It's a clever way to upscale images using learned convolutions instead of \n",
    "    simple interpolation.\n",
    "    \n",
    "    How it works:\n",
    "    - Input:  (batch, channels * r^2, height, width)\n",
    "    - Output: (batch, channels, height * r, width * r)\n",
    "    \n",
    "    where r is the upscale factor.\n",
    "    \n",
    "    Example with r=2:\n",
    "    - Input:  (B, 12, 100, 100) where 12 = 3 * 2^2\n",
    "    - Output: (B, 3, 200, 200)\n",
    "    \n",
    "    The magic is in the reshaping and permutation that rearranges the channel\n",
    "    data into spatial locations.\n",
    "    \"\"\"\n",
    "    def __init__(self, upscale_factor):\n",
    "        super(CustomPixelShuffle, self).__init__()\n",
    "        self.upscale_factor = upscale_factor\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Rearrange tensor to increase spatial resolution.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (batch, channels * r², height, width)\n",
    "            \n",
    "        Returns:\n",
    "            Output tensor of shape (batch, channels, height * r, width * r)\n",
    "        \"\"\"\n",
    "        r = self.upscale_factor\n",
    "        \n",
    "        # Get input dimensions\n",
    "        b, c, h, w = x.shape\n",
    "        \n",
    "        # Step 1: Reshape to separate the upscale factor from channels\n",
    "        # (b, c, h, w) -> (b, c/r², r, r, h, w)\n",
    "        # This groups the channels that will become spatial pixels\n",
    "        x = x.reshape(b, c // (r**2), r, r, h, w)\n",
    "        \n",
    "        # Step 2: Permute dimensions to interleave spatial information\n",
    "        # (b, c/r², r, r, h, w) -> (b, c/r², h, r, w, r)\n",
    "        # This moves the r×r blocks next to their spatial positions\n",
    "        x = x.permute(0, 1, 4, 2, 5, 3)\n",
    "        \n",
    "        # Step 3: Collapse the r dimensions into spatial dimensions\n",
    "        # (b, c/r², h, r, w, r) -> (b, c/r², h*r, w*r)\n",
    "        # Now we have increased spatial resolution!\n",
    "        x = x.reshape(b, c // (r**2), h * r, w * r)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "66d841fb-b5bc-4d93-99fe-694a3e887996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SUPER RESOLUTION CNN\n",
    "# =============================================================================\n",
    "   \n",
    "class SuperResCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    A Convolutional Neural Network for 2x image super-resolution.\n",
    "    \n",
    "    Architecture:\n",
    "    - Feature extraction: 3 convolutional layers to learn image patterns\n",
    "    - Upsampling: Sub-pixel convolution (PixelShuffle) to increase resolution\n",
    "    - Refinement: Final conv layer to polish the output\n",
    "    \n",
    "    Input:  3-channel RGB image of size 225x300\n",
    "    Output: 3-channel RGB image of size 450x600 (2x larger)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(SuperResCNN, self).__init__()\n",
    "        # Feature extraction layers - learn to recognize patterns like edges and textures\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=9, padding=4)   # First conv layer\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=5, padding=2)   # Second conv layer\n",
    "        self.conv3 = nn.Conv2d(64, 32, kernel_size=5, padding=2)   # Third conv layer\n",
    "        \n",
    "        # Upsampling branch - increases image resolution by 2x\n",
    "        self.upsample_conv = nn.Conv2d(32, 12, kernel_size=3, padding=1)\n",
    "        # We could use PyTorch's implementation of PixelShuffle\n",
    "        # self.pixel_shuffle = nn.PixelShuffle(upscale_factor=2)      # Rearranges channels to spatial dimensions\n",
    "        # But we will be using our own implementation of PixelShuffle\n",
    "        self.pixel_shuffle = CustomPixelShuffle(upscale_factor=2)\n",
    "        \n",
    "        # Final refinement layer - smooths out artifacts\n",
    "        self.refine = nn.Conv2d(3, 3, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the network\n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, 3, 225, 300)\n",
    "        Returns:\n",
    "            Output tensor of shape (batch_size, 3, 450, 600)\n",
    "        \"\"\"\n",
    "        # Feature extraction with ReLU activation\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        \n",
    "        # Upsampling to increase resolution\n",
    "        x = self.upsample_conv(x)\n",
    "        x = self.pixel_shuffle(x)\n",
    "        \n",
    "        # Final refinement\n",
    "        x = self.refine(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28304979-b897-4d42-8dd3-5df22e5b22fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LOAD THE TRAINED MODEL\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LOADING TRAINED MODEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create model instance and load saved weights\n",
    "model = SuperResCNN().to(device)\n",
    "\n",
    "try:\n",
    "    # Attempt to load the best saved model weights in training onto the appropriate device (GPU or CPU)\n",
    "    model.load_state_dict(torch.load('best_superres_model.pth', map_location=device))\n",
    "    \n",
    "    # Set the model to evaluation mode (disables dropout/batchnorm training behavior)\n",
    "    model.eval()\n",
    "    \n",
    "    # Print confirmation message with parameter count\n",
    "    print(f\"Model loaded successfully with {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"Warning: 'best_superres_model.pth' not found. Using the current model state.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}. Using the current model state.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [CS373]",
   "language": "python",
   "name": "cs373"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
